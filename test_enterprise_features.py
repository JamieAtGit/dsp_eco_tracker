#!/usr/bin/env python3
"""
üß™ ENTERPRISE FEATURES TESTING SUITE
===================================

Comprehensive testing script to validate all new enterprise components:
- Error handling framework
- Monitoring and observability  
- Caching with circuit breaker
- Rate limiting and authentication
- API documentation generation
- ML model monitoring and drift detection

This script provides end-to-end testing of the production-ready features
to ensure they work correctly before demonstration.
"""

import asyncio
import time
import json
import requests
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnterpriseFeatureTester:
    """
    Comprehensive tester for all enterprise features
    
    Tests each component systematically with realistic scenarios
    and provides detailed reports on functionality.
    """
    
    def __init__(self):
        self.test_results = {
            "error_handling": {},
            "monitoring": {},
            "caching": {},
            "rate_limiting": {},
            "api_docs": {},
            "ml_monitoring": {}
        }
        
        # Test configuration
        self.base_url = "http://localhost:5000"
        self.test_api_key = None
        self.test_jwt_token = None
        
        print("üß™ Enterprise Features Testing Suite Initialized")
        print("=" * 60)
    
    async def run_all_tests(self):
        """Run comprehensive test suite for all enterprise features"""
        
        print("üöÄ Starting Enterprise Features Test Suite")
        print("=" * 60)
        
        # Test each component
        await self.test_error_handling()
        await self.test_monitoring_system()
        await self.test_caching_layer()
        await self.test_rate_limiting()
        await self.test_api_documentation()
        await self.test_ml_monitoring()
        
        # Generate comprehensive report
        self.generate_test_report()
    
    async def test_error_handling(self):
        """Test the comprehensive error handling framework"""
        
        print("\nüö® Testing Error Handling Framework")
        print("-" * 40)
        
        try:
            # Import the error handling components
            from backend.core.exceptions import (
                BaseEcoTrackerException,
                ScrapingException,
                DataValidationException,
                MLModelException,
                ConfigurationException,
                RateLimitException,
                ErrorHandler,
                error_handler
            )
            
            results = {}
            
            # Test 1: Custom exception creation
            print("1Ô∏è‚É£ Testing custom exception creation...")
            try:
                raise ScrapingException(
                    "Test scraping failure",
                    url="https://test.com",
                    strategy="requests",
                    http_status=403
                )
            except ScrapingException as e:
                results["custom_exception"] = {
                    "success": True,
                    "exception_id": e.exception_id,
                    "severity": e.severity,
                    "category": e.category,
                    "recovery_suggestion": e.recovery_suggestion
                }
                print(f"   ‚úÖ Exception created with ID: {e.exception_id}")
                print(f"   üìä Context: {len(e.get_context())} fields")
            
            # Test 2: Error handler processing
            print("2Ô∏è‚É£ Testing error handler processing...")
            try:
                validation_error = DataValidationException(
                    "Invalid postcode format",
                    field_name="postcode",
                    field_value="INVALID",
                    validation_rule="uk_postcode_format"
                )
                
                response = error_handler.handle_exception(validation_error)
                results["error_handler"] = {
                    "success": True,
                    "response_keys": list(response.keys()),
                    "user_friendly": "error" in response and "message" in response
                }
                print(f"   ‚úÖ Error handler response: {list(response.keys())}")
                
            except Exception as e:
                results["error_handler"] = {"success": False, "error": str(e)}
                print(f"   ‚ùå Error handler test failed: {e}")
            
            # Test 3: Error statistics
            print("3Ô∏è‚É£ Testing error statistics...")
            stats = error_handler.get_error_statistics()
            results["error_stats"] = {
                "success": True,
                "total_errors": stats["total_errors"],
                "has_breakdown": "error_breakdown" in stats,
                "has_top_errors": "top_errors" in stats
            }
            print(f"   ‚úÖ Error statistics: {stats['total_errors']} total errors")
            
            self.test_results["error_handling"] = results
            print("‚úÖ Error handling framework tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import error handling components: {e}")
            self.test_results["error_handling"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå Error handling tests failed: {e}")
            self.test_results["error_handling"] = {"success": False, "error": str(e)}
    
    async def test_monitoring_system(self):
        """Test the OpenTelemetry monitoring and observability system"""
        
        print("\nüìä Testing Monitoring & Observability System")
        print("-" * 40)
        
        try:
            from backend.core.monitoring import monitoring, trace_scraping, trace_ml_prediction
            
            results = {}
            
            # Test 1: Basic monitoring initialization
            print("1Ô∏è‚É£ Testing monitoring initialization...")
            health_status = monitoring.get_health_status()
            results["initialization"] = {
                "success": True,
                "service_name": health_status["service"],
                "uptime": health_status["uptime_seconds"] > 0,
                "total_requests": health_status["total_requests"]
            }
            print(f"   ‚úÖ Service: {health_status['service']}")
            print(f"   ‚è±Ô∏è Uptime: {health_status['uptime_readable']}")
            
            # Test 2: Trace operation decorator
            print("2Ô∏è‚É£ Testing trace operation decorator...")
            
            @trace_scraping("test_strategy")
            def test_scraping_operation():
                time.sleep(0.1)  # Simulate work
                return {"status": "success", "data": "test_data"}
            
            start_time = time.time()
            result = test_scraping_operation()
            duration = time.time() - start_time
            
            results["trace_decorator"] = {
                "success": result["status"] == "success",
                "duration_reasonable": 0.1 <= duration <= 0.2,
                "result_data": result.get("data") == "test_data"
            }
            print(f"   ‚úÖ Traced operation completed in {duration:.3f}s")
            
            # Test 3: Metrics recording
            print("3Ô∏è‚É£ Testing metrics recording...")
            
            # Record various metrics
            monitoring.record_request("/test", "GET", 200, 0.1)
            monitoring.record_scraping_result(True, "requests", 0.5)
            monitoring.record_ml_prediction("xgboost", 0.95, 0.02)
            monitoring.record_business_metrics(2.5, 150.0, "truck")
            
            results["metrics_recording"] = {
                "success": True,
                "metrics_recorded": ["request", "scraping", "ml_prediction", "business"]
            }
            print("   ‚úÖ All metric types recorded successfully")
            
            # Test 4: Alert creation
            print("4Ô∏è‚É£ Testing alert creation...")
            monitoring.create_alert(
                "test_alert",
                "medium",
                "Test alert for validation",
                {"test_key": "test_value"}
            )
            
            results["alert_creation"] = {"success": True}
            print("   ‚úÖ Alert created and logged")
            
            self.test_results["monitoring"] = results
            print("‚úÖ Monitoring system tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import monitoring components: {e}")
            self.test_results["monitoring"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå Monitoring tests failed: {e}")
            self.test_results["monitoring"] = {"success": False, "error": str(e)}
    
    async def test_caching_layer(self):
        """Test the Redis caching system with circuit breaker"""
        
        print("\nüöÄ Testing Caching Layer with Circuit Breaker")
        print("-" * 40)
        
        try:
            from backend.core.caching import cache, cache_for, CacheService
            
            results = {}
            
            # Test 1: Basic cache operations
            print("1Ô∏è‚É£ Testing basic cache operations...")
            
            # Test cache set and get
            test_key = "test_cache_key"
            test_value = {"data": "test_value", "timestamp": datetime.utcnow().isoformat()}
            
            set_success = await cache.set(test_key, test_value, ttl=60)
            retrieved_value = await cache.get(test_key)
            
            results["basic_operations"] = {
                "set_success": set_success,
                "get_success": retrieved_value is not None,
                "data_integrity": retrieved_value == test_value if retrieved_value else False
            }
            
            if retrieved_value:
                print(f"   ‚úÖ Cache SET/GET successful - Data integrity: {retrieved_value == test_value}")
            else:
                print("   ‚ö†Ô∏è Cache operations may be failing (Redis not available?)")
            
            # Test 2: Cache decorator
            print("2Ô∏è‚É£ Testing cache decorator...")
            
            @cache_for(30)
            async def expensive_calculation(x, y):
                await asyncio.sleep(0.1)  # Simulate expensive operation
                return {"result": x + y, "computed_at": time.time()}
            
            # First call - should be slow
            start_time = time.time()
            result1 = await expensive_calculation(5, 3)
            first_call_time = time.time() - start_time
            
            # Second call - should be fast (cached)
            start_time = time.time()
            result2 = await expensive_calculation(5, 3)
            second_call_time = time.time() - start_time
            
            results["cache_decorator"] = {
                "first_call_slow": first_call_time >= 0.1,
                "second_call_fast": second_call_time < 0.05,
                "results_identical": result1 == result2,
                "speedup": first_call_time / second_call_time if second_call_time > 0 else 0
            }
            
            print(f"   ‚úÖ First call: {first_call_time:.3f}s, Second call: {second_call_time:.3f}s")
            print(f"   üöÄ Cache speedup: {results['cache_decorator']['speedup']:.1f}x")
            
            # Test 3: Health check
            print("3Ô∏è‚É£ Testing cache health check...")
            health = await cache.health_check()
            
            results["health_check"] = {
                "status": health["status"],
                "response_time_reasonable": health["response_time_ms"] < 100,
                "operations_tested": health.get("operations_tested", [])
            }
            print(f"   ‚úÖ Health status: {health['status']} ({health['response_time_ms']:.2f}ms)")
            
            # Test 4: Cache statistics
            print("4Ô∏è‚É£ Testing cache statistics...")
            stats = await cache.get_stats()
            
            results["statistics"] = {
                "has_hit_rate": "hit_rate" in stats,
                "has_redis_info": "redis_info" in stats,
                "hit_rate": stats.get("hit_rate", 0)
            }
            print(f"   ‚úÖ Cache hit rate: {stats.get('hit_rate', 0)}%")
            
            self.test_results["caching"] = results
            print("‚úÖ Caching layer tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import caching components: {e}")
            self.test_results["caching"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå Caching tests failed: {e}")
            self.test_results["caching"] = {"success": False, "error": str(e)}
    
    async def test_rate_limiting(self):
        """Test the API rate limiting and authentication system"""
        
        print("\nüõ°Ô∏è Testing Rate Limiting & Authentication")
        print("-" * 40)
        
        try:
            from backend.core.rate_limiting import (
                RateLimitService,
                AuthenticationService,
                UserRole,
                TokenBucket,
                SlidingWindowCounter
            )
            import redis.asyncio as redis
            
            results = {}
            
            # Initialize Redis client for testing
            redis_client = redis.Redis.from_url("redis://localhost:6379", decode_responses=True)
            
            # Test 1: Authentication service
            print("1Ô∏è‚É£ Testing authentication service...")
            
            auth_service = AuthenticationService("test-secret-key", redis_client)
            
            # Generate JWT token
            jwt_token = auth_service.generate_jwt_token(
                "test_user_123",
                UserRole.PREMIUM,
                {"test_claim": "test_value"}
            )
            
            # Verify JWT token
            try:
                payload = auth_service.verify_jwt_token(jwt_token)
                results["jwt_auth"] = {
                    "token_generated": len(jwt_token) > 0,
                    "token_verified": payload["user_id"] == "test_user_123",
                    "role_correct": payload["role"] == UserRole.PREMIUM,
                    "claims_preserved": payload.get("test_claim") == "test_value"
                }
                print(f"   ‚úÖ JWT token generated and verified for user: {payload['user_id']}")
                self.test_jwt_token = jwt_token
                
            except Exception as e:
                results["jwt_auth"] = {"success": False, "error": str(e)}
                print(f"   ‚ùå JWT authentication failed: {e}")
            
            # Generate API key
            try:
                api_key, key_id = await auth_service.generate_api_key(
                    "test_user_123",
                    UserRole.STANDARD,
                    "test_key"
                )
                
                # Verify API key
                key_info = await auth_service.verify_api_key(api_key)
                
                results["api_key_auth"] = {
                    "key_generated": len(api_key) > 0,
                    "key_verified": key_info["user_id"] == "test_user_123",
                    "role_correct": key_info["role"] == UserRole.STANDARD
                }
                print(f"   ‚úÖ API key generated: {key_id}")
                self.test_api_key = api_key
                
            except Exception as e:
                results["api_key_auth"] = {"success": False, "error": str(e)}
                print(f"   ‚ùå API key authentication failed: {e}")
            
            # Test 2: Token bucket algorithm
            print("2Ô∏è‚É£ Testing token bucket rate limiting...")
            
            bucket = TokenBucket(
                capacity=5,
                refill_rate=1.0,  # 1 token per second
                redis_client=redis_client,
                key_prefix="test_bucket"  
            )
            
            # Consume tokens rapidly
            successful_requests = 0
            for i in range(10):
                allowed, metadata = await bucket.consume("test_user", 1)
                if allowed:
                    successful_requests += 1
            
            results["token_bucket"] = {
                "initial_burst_allowed": successful_requests >= 5,
                "rate_limiting_active": successful_requests < 10,
                "metadata_provided": "tokens_remaining" in metadata
            }
            print(f"   ‚úÖ Token bucket: {successful_requests}/10 requests allowed")
            
            # Test 3: Sliding window algorithm
            print("3Ô∏è‚É£ Testing sliding window rate limiting...")
            
            window = SlidingWindowCounter(
                window_size=60,  # 1 minute
                max_requests=3,
                redis_client=redis_client,
                key_prefix="test_window"
            )
            
            window_successful = 0
            for i in range(5):
                allowed, metadata = await window.is_allowed("test_user_window")
                if allowed:
                    window_successful += 1
                await asyncio.sleep(0.1)  # Small delay
            
            results["sliding_window"] = {
                "requests_allowed": window_successful,
                "rate_limiting_active": window_successful <= 3,
                "metadata_provided": "requests_in_window" in metadata
            }
            print(f"   ‚úÖ Sliding window: {window_successful}/5 requests allowed")
            
            # Test 4: Rate limit service
            print("4Ô∏è‚É£ Testing rate limit service...")
            
            rate_limit_service = RateLimitService(redis_client)
            
            # Test rate limiting for different roles
            for role in [UserRole.GUEST, UserRole.STANDARD, UserRole.PREMIUM]:
                allowed, metadata = await rate_limit_service.check_rate_limit(
                    f"test_user_{role}",
                    role,
                    "test_endpoint"
                )
                
                results[f"rate_limit_{role}"] = {
                    "allowed": allowed,
                    "has_metadata": len(metadata) > 0,
                    "algorithm_specified": "algorithm" in metadata
                }
                print(f"   ‚úÖ Rate limit check for {role}: {'Allowed' if allowed else 'Blocked'}")
            
            await redis_client.close()
            
            self.test_results["rate_limiting"] = results
            print("‚úÖ Rate limiting tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import rate limiting components: {e}")
            self.test_results["rate_limiting"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå Rate limiting tests failed: {e}")
            self.test_results["rate_limiting"] = {"success": False, "error": str(e)}
    
    async def test_api_documentation(self):
        """Test the OpenAPI documentation generation system"""
        
        print("\nüìö Testing API Documentation System")
        print("-" * 40)
        
        try:
            from backend.core.api_documentation import (
                OpenAPIDocumentationGenerator,
                APIEndpoint,
                APISchema,
                api_docs
            )
            
            results = {}
            
            # Test 1: OpenAPI spec generation
            print("1Ô∏è‚É£ Testing OpenAPI spec generation...")
            
            # Add a test schema
            test_schema = APISchema(
                name="TestRequest",
                properties={
                    "test_field": {"type": "string"},
                    "number_field": {"type": "integer"}
                },
                required=["test_field"],
                example={"test_field": "example", "number_field": 42}
            )
            api_docs.add_schema(test_schema)
            
            # Generate OpenAPI spec
            spec = api_docs.generate_openapi_spec()
            
            results["openapi_spec"] = {
                "has_info": "info" in spec,
                "has_paths": "paths" in spec,
                "has_components": "components" in spec,
                "has_schemas": "schemas" in spec.get("components", {}),
                "openapi_version": spec.get("openapi") == "3.0.0",
                "test_schema_included": "TestRequest" in spec.get("components", {}).get("schemas", {})
            }
            
            print(f"   ‚úÖ OpenAPI spec generated: {spec['info']['title']} v{spec['info']['version']}")
            print(f"   üìã Schemas: {len(spec.get('components', {}).get('schemas', {}))}")
            print(f"   üõ§Ô∏è Paths: {len(spec.get('paths', {}))}")
            
            # Test 2: Postman collection export
            print("2Ô∏è‚É£ Testing Postman collection export...")
            
            postman_collection = api_docs.export_postman_collection()
            
            results["postman_export"] = {
                "has_info": "info" in postman_collection,
                "has_items": "item" in postman_collection,
                "has_auth": "auth" in postman_collection,
                "collection_name": postman_collection.get("info", {}).get("name")
            }
            
            print(f"   ‚úÖ Postman collection: {postman_collection['info']['name']}")
            print(f"   üìÅ Folders: {len(postman_collection.get('item', []))}")
            
            # Test 3: Documentation decorator
            print("3Ô∏è‚É£ Testing documentation decorator...")
            
            @api_docs.document_endpoint(
                summary="Test endpoint",
                description="Test endpoint for validation",
                tags=["testing"],
                examples={"test_example": {"field": "value"}}
            )
            def test_endpoint():
                return {"status": "success"}
            
            # Execute decorated function
            result = test_endpoint()
            
            results["documentation_decorator"] = {
                "function_executable": result["status"] == "success",
                "decorator_applied": hasattr(api_docs, '_pending_docs')
            }
            
            print("   ‚úÖ Documentation decorator applied successfully")
            
            self.test_results["api_docs"] = results
            print("‚úÖ API documentation tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import API documentation components: {e}")
            self.test_results["api_docs"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå API documentation tests failed: {e}")
            self.test_results["api_docs"] = {"success": False, "error": str(e)}
    
    async def test_ml_monitoring(self):
        """Test the ML model monitoring and drift detection system"""
        
        print("\nü§ñ Testing ML Monitoring & Drift Detection")
        print("-" * 40)
        
        try:
            from backend.core.ml_monitoring import (
                MLMonitoringService,
                DriftDetector,
                ModelPerformanceMonitor,
                StatisticalTests,
                DriftType,
                AlertSeverity
            )
            
            results = {}
            
            # Test 1: ML monitoring service initialization
            print("1Ô∏è‚É£ Testing ML monitoring service...")
            
            ml_monitor = MLMonitoringService("test_model", "1.0.0")
            
            # Set baseline data
            baseline_data = pd.DataFrame({
                'feature_1': np.random.normal(0, 1, 1000),
                'feature_2': np.random.normal(5, 2, 1000),
                'feature_3': np.random.exponential(2, 1000)
            })
            ml_monitor.set_baseline_data(baseline_data)
            
            # Set baseline performance
            ml_monitor.set_baseline_performance({
                'accuracy': 0.85,
                'precision': 0.83,
                'recall': 0.87,
                'f1': 0.85
            })
            
            results["ml_monitoring_init"] = {
                "service_initialized": ml_monitor.model_name == "test_model",
                "baseline_data_set": len(ml_monitor.drift_detector.reference_data) > 0,
                "baseline_performance_set": ml_monitor.performance_monitor.baseline_accuracy == 0.85
            }
            
            print(f"   ‚úÖ ML monitoring initialized for {ml_monitor.model_name}")
            print(f"   üìä Baseline features: {len(baseline_data.columns)}")
            
            # Test 2: Prediction logging
            print("2Ô∏è‚É£ Testing prediction logging...")
            
            predictions_logged = 0
            for i in range(50):
                features = {
                    'feature_1': np.random.normal(0, 1),
                    'feature_2': np.random.normal(5, 2),
                    'feature_3': np.random.exponential(2)
                }
                
                prediction_id = ml_monitor.log_prediction(
                    input_features=features,
                    prediction=np.random.choice([0, 1]),
                    confidence=np.random.uniform(0.7, 0.95),
                    processing_time_ms=np.random.uniform(10, 50),
                    user_id=f"user_{i % 10}"
                )
                
                if prediction_id:
                    predictions_logged += 1
                
                # Add feedback for some predictions
                if i % 5 == 0:
                    ml_monitor.add_feedback(prediction_id, np.random.choice([0, 1]))
            
            results["prediction_logging"] = {
                "predictions_logged": predictions_logged,
                "all_successful": predictions_logged == 50,
                "prediction_ids_generated": len(ml_monitor.predictions) > 0
            }
            
            print(f"   ‚úÖ Logged {predictions_logged}/50 predictions")
            
            # Test 3: Drift detection with artificial drift
            print("3Ô∏è‚É£ Testing drift detection...")
            
            drift_detector = DriftDetector()
            
            # Add reference data (normal distribution)
            reference_data = np.random.normal(0, 1, 1000)
            drift_detector.add_reference_data("test_feature", reference_data.tolist())
            
            # Add current data with drift (shifted distribution)
            for _ in range(150):  # Enough to trigger detection
                drifted_value = np.random.normal(2, 1)  # Mean shifted from 0 to 2
                drift_detector.add_current_data("test_feature", drifted_value)
            
            results["drift_detection"] = {
                "reference_data_added": len(drift_detector.reference_data["test_feature"]) > 0,
                "current_data_added": len(drift_detector.current_data["test_feature"]) > 0,
                "alerts_generated": len(drift_detector.alerts) > 0
            }
            
            if drift_detector.alerts:
                alert = drift_detector.alerts[-1]
                print(f"   üö® Drift detected: {alert.message}")
                print(f"   üìä Drift score: {alert.drift_score:.3f}")
            else:
                print("   ‚ö†Ô∏è No drift alerts generated (may need more data)")
            
            # Test 4: Statistical tests
            print("4Ô∏è‚É£ Testing statistical tests...")
            
            # KS test
            reference = np.random.normal(0, 1, 1000)
            drifted = np.random.normal(1, 1, 1000)  # Mean shift
            
            ks_p_value, ks_drift = StatisticalTests.kolmogorov_smirnov_test(reference, drifted)
            psi_score = StatisticalTests.population_stability_index(reference, drifted)
            kl_divergence = StatisticalTests.kullback_leibler_divergence(reference, drifted)
            
            results["statistical_tests"] = {
                "ks_test_detects_drift": ks_drift,
                "ks_p_value_reasonable": 0 <= ks_p_value <= 1,
                "psi_score_positive": psi_score > 0,
                "kl_divergence_positive": kl_divergence > 0
            }
            
            print(f"   ‚úÖ KS test: p-value={ks_p_value:.4f}, drift={'detected' if ks_drift else 'not detected'}")
            print(f"   ‚úÖ PSI score: {psi_score:.4f}")
            print(f"   ‚úÖ KL divergence: {kl_divergence:.4f}")
            
            # Test 5: Monitoring dashboard
            print("5Ô∏è‚É£ Testing monitoring dashboard...")
            
            dashboard = ml_monitor.get_monitoring_dashboard()
            
            results["monitoring_dashboard"] = {
                "has_model_info": "model_info" in dashboard,
                "has_drift_status": "drift_status" in dashboard,
                "has_feature_stats": "feature_statistics" in dashboard,
                "has_confidence_dist": "confidence_distribution" in dashboard,
                "total_predictions": dashboard.get("model_info", {}).get("total_predictions", 0)
            }
            
            print(f"   ‚úÖ Dashboard generated with {len(dashboard)} sections")
            print(f"   üìä Total predictions tracked: {dashboard.get('model_info', {}).get('total_predictions', 0)}")
            
            self.test_results["ml_monitoring"] = results
            print("‚úÖ ML monitoring tests completed")
            
        except ImportError as e:
            print(f"‚ùå Failed to import ML monitoring components: {e}")
            self.test_results["ml_monitoring"] = {"success": False, "error": str(e)}
        except Exception as e:
            print(f"‚ùå ML monitoring tests failed: {e}")
            self.test_results["ml_monitoring"] = {"success": False, "error": str(e)}
    
    def generate_test_report(self):
        """Generate comprehensive test report"""
        
        print("\n" + "=" * 60)
        print("üìã ENTERPRISE FEATURES TEST REPORT")
        print("=" * 60)
        
        total_tests = 0
        passed_tests = 0
        
        for component, results in self.test_results.items():
            print(f"\nüîß {component.upper().replace('_', ' ')}")
            print("-" * 30)
            
            if isinstance(results, dict) and "success" in results and results["success"] is False:
                print(f"‚ùå Component failed to load: {results.get('error', 'Unknown error')}")
                total_tests += 1
                continue
            
            component_tests = 0
            component_passed = 0
            
            for test_name, test_result in results.items():
                if isinstance(test_result, dict):
                    component_tests += 1
                    total_tests += 1
                    
                    # Check if test passed (most values are True or positive)
                    if isinstance(test_result, dict):
                        success_indicators = [
                            v for v in test_result.values() 
                            if isinstance(v, bool) and v
                        ]
                        if len(success_indicators) >= len(test_result) * 0.7:  # 70% success rate
                            component_passed += 1
                            passed_tests += 1
                            status = "‚úÖ"
                        else:
                            status = "‚ö†Ô∏è"
                    else:
                        status = "‚ùì"
                    
                    print(f"  {status} {test_name}: {self._summarize_test_result(test_result)}")
            
            if component_tests > 0:
                success_rate = (component_passed / component_tests) * 100
                print(f"  üìä Component Success Rate: {success_rate:.1f}% ({component_passed}/{component_tests})")
        
        print("\n" + "=" * 60)
        print("üéØ OVERALL TEST SUMMARY")
        print("=" * 60)
        
        if total_tests > 0:
            overall_success_rate = (passed_tests / total_tests) * 100
            print(f"‚úÖ Tests Passed: {passed_tests}/{total_tests} ({overall_success_rate:.1f}%)")
            
            if overall_success_rate >= 90:
                print("üèÜ EXCELLENT: Enterprise features are production-ready!")
            elif overall_success_rate >= 75:
                print("üéØ GOOD: Most enterprise features working, minor issues to address")
            elif overall_success_rate >= 50:
                print("‚ö†Ô∏è MODERATE: Several issues need attention before production")
            else:
                print("‚ùå CRITICAL: Major issues with enterprise features")
        else:
            print("‚ùå No tests were able to run - check component imports")
        
        print("\nüöÄ Enterprise features testing completed!")
        
        # Save detailed results to file
        with open("enterprise_test_results.json", "w") as f:
            json.dump(self.test_results, f, indent=2, default=str)
        print("üìÑ Detailed results saved to: enterprise_test_results.json")
    
    def _summarize_test_result(self, result: Dict[str, Any]) -> str:
        """Create a summary of test result"""
        if not isinstance(result, dict):
            return str(result)
        
        positive_count = sum(1 for v in result.values() if v is True or (isinstance(v, (int, float)) and v > 0))
        total_count = len(result)
        
        if positive_count >= total_count * 0.8:
            return f"All checks passed ({positive_count}/{total_count})"
        elif positive_count >= total_count * 0.5:
            return f"Most checks passed ({positive_count}/{total_count})"
        else:
            return f"Issues detected ({positive_count}/{total_count})"

async def main():
    """Main test execution function"""
    
    print("üß™ STARTING ENTERPRISE FEATURES COMPREHENSIVE TEST")
    print("=" * 60)
    print("This will test all new enterprise components:")
    print("‚Ä¢ Error handling framework")
    print("‚Ä¢ Monitoring and observability")  
    print("‚Ä¢ Caching with circuit breaker")
    print("‚Ä¢ Rate limiting and authentication")
    print("‚Ä¢ API documentation generation")
    print("‚Ä¢ ML model monitoring and drift detection")
    print("=" * 60)
    
    tester = EnterpriseFeatureTester()
    await tester.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())